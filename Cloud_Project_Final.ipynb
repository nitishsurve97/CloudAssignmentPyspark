{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType, FloatType, MapType, TimestampType\n",
    "from pyspark.sql import functions as F, SparkSession, types as T\n",
    "from pyspark.sql.functions import col, lit, udf, from_json, year, month, expr\n",
    "from pyspark.sql.window import Window\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Enviornment Variable and necessary connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\\\Spark\\\\spark-3.5.3-bin-hadoop3\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"D:\\\\hadoop3\"\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(os.environ[\"HADOOP_HOME\"], \"bin\")\n",
    "\n",
    "\n",
    "# Create SparkSession\n",
    "mysql_driver_path = \"C:\\\\mysql-connector-j-9.1.0\\\\mysql-connector-j-9.1.0.jar\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"cloudProject\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.jars\", mysql_driver_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "## DB url\n",
    "url = \"jdbc:mysql://localhost:3306/dummy\"\n",
    "\n",
    "## DB connection String\n",
    "path = 'db_config.json'\n",
    "with open(path, 'r') as f:\n",
    "    db_config = json.load(f)\n",
    "    \n",
    "db_cnx = create_engine(f\"mysql+pymysql://{db_config['user']}:{db_config['password']}@{db_config['ip']}/{db_config['db_name']}\", \n",
    "                       pool_recycle=3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stock_data(source_name, api_key):\n",
    "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={source_name}&outputsize=full&apikey={api_key}'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_get_inc_df(spark, url, stock_data_df, source_name, db_config):\n",
    "    qry = f\"SELECT last_refresh_dt FROM source_audit WHERE source_name = '{source_name}'\"\n",
    "    \n",
    "    last_refresh_df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"user\", db_config['user']) \\\n",
    "        .option(\"password\", db_config['password']) \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .option(\"query\", qry) \\\n",
    "        .load()\n",
    "    \n",
    "    last_refresh_dt = last_refresh_df.collect()[0]['last_refresh_dt']\n",
    "    last_refresh_dt = datetime.strptime(last_refresh_dt, '%Y-%m-%d').date()\n",
    "    filtered_df = stock_data_df[stock_data_df['trade_dt'] >= last_refresh_dt]\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_insert_stock_date(df, source_name, db_cnx):\n",
    "    df['stock_info'] = df['stock_info'].apply(json.dumps)\n",
    "    \n",
    "    source_name = source_name.lower()\n",
    "    df.to_sql(f'st_stock_{source_name}_info', db_cnx, if_exists='replace', index=False)\n",
    "    \n",
    "    db_procedure_name = f'sp_stock_{source_name}'\n",
    "    with db_cnx.connect() as connection:\n",
    "        connection.execute(text(f'CALL {db_procedure_name}()'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_source_audit(spark, url, db_config, source_name, last_refresh_dt, last_exec_dt, tot_exec_time, db_cnx):\n",
    "#     source_name = source_name.upper()\n",
    "# #     print(source_name)\n",
    "\n",
    "#     source_audit_df = spark.read \\\n",
    "#         .format(\"jdbc\") \\\n",
    "#         .option(\"url\", url) \\\n",
    "#         .option(\"dbtable\", \"source_audit\") \\\n",
    "#         .option(\"user\", db_config['user']) \\\n",
    "#         .option(\"password\", db_config['password']) \\\n",
    "#         .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "#         .load()\n",
    "    \n",
    "#     updated_df = source_audit_df.filter(source_audit_df['source_name'] == source_name)\n",
    "\n",
    "#     ## altering only specific row , according to condition\n",
    "#     updated_df = source_audit_df.withColumn(\n",
    "#         'last_refresh_dt', \n",
    "#         F.when(source_audit_df['source_name'] == source_name, F.lit(last_refresh_dt))\n",
    "#         .otherwise(source_audit_df['last_refresh_dt'])\n",
    "#     ).withColumn(\n",
    "#         'last_exec_dt',\n",
    "#         F.when(source_audit_df['source_name'] == source_name, F.lit(last_exec_dt))\n",
    "#         .otherwise(source_audit_df['last_exec_dt'])\n",
    "#     ).withColumn(\n",
    "#         'tot_exec_time',\n",
    "#         F.when(source_audit_df['source_name'] == source_name, F.lit(tot_exec_time))\n",
    "#         .otherwise(source_audit_df['tot_exec_time'])\n",
    "#     )\n",
    "\n",
    "    \n",
    "#     updated_df.write \\\n",
    "#         .format(\"jdbc\") \\\n",
    "#         .option(\"url\", url) \\\n",
    "#         .option(\"dbtable\", \"source_audit\") \\\n",
    "#         .option(\"user\", db_config['user']) \\\n",
    "#         .option(\"password\", db_config['password']) \\\n",
    "#         .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "#         .mode(\"overwrite\") \\\n",
    "#         .save()\n",
    "    \n",
    "#     ## This solution is not the most optimal .Pyspark does not natively support Update opertiaon,\n",
    "#     # so the typical approach involves overwriting the entire table or partition\n",
    "\n",
    "\n",
    "def fn_update_source_audit(source_name, last_refresh_dt, last_exec_dt, tot_exec_time, db_cnx):\n",
    "    qry = text(\"\"\"\n",
    "        UPDATE source_audit\n",
    "        SET \n",
    "            last_refresh_dt = :last_refresh_dt,\n",
    "            last_exec_dt = :last_exec_dt,\n",
    "            tot_exec_time = :tot_exec_time\n",
    "        WHERE source_name = :source_name\n",
    "    \"\"\")\n",
    "    \n",
    "    # Execute the query with parameters as a dictionary\n",
    "    with db_cnx.connect() as connection:\n",
    "        connection.execute(qry, {\n",
    "            'last_refresh_dt': last_refresh_dt,\n",
    "            'last_exec_dt': last_exec_dt,\n",
    "            'tot_exec_time': tot_exec_time,\n",
    "            'source_name': source_name\n",
    "        })\n",
    "        connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_insert_task_audit(source_name,last_exec_dt,exec_time, db_cnx):\n",
    "    source_name = source_name.upper()\n",
    "    task_dict = {\n",
    "        'task_name' : {source_name},\n",
    "        'last_exec_dt' : {last_exec_dt},\n",
    "        'flg_success' : 1,\n",
    "        'exec_time' : {exec_time}\n",
    "    }\n",
    "    df = pd.DataFrame([task_dict])\n",
    "    df.to_sql('task_audit', db_cnx, if_exists='append', index=False)\n",
    "    \n",
    "def fn_insert_source_info(source_name, last_refresh_dt, time_zone, info):\n",
    "    source_name = source_name.upper()\n",
    "    source_dict = {\n",
    "        'source_name': [source_name],  \n",
    "        'last_refresh_dt': [last_refresh_dt],  \n",
    "        'time_zone': ['US/Eastern'],  \n",
    "        'info': [info] \n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(source_dict)\n",
    "\n",
    "    df.to_sql('source_info', db_cnx, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_fetch_data(source_name, spark, db_config, url, db_cnx):\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    ## Never share this key\n",
    "    dcu_mail_api_key = '410VUNUTN1PD45LW'\n",
    "    #personal_mail_api_key = 'J9M3DNRBBYIK9GXN'\n",
    "\n",
    "    data = fetch_stock_data(source_name, dcu_mail_api_key)\n",
    "    \n",
    "    info_df = pd.DataFrame(data).reset_index()\n",
    "    last_refresh_dt = info_df.loc[info_df['index'].str.contains('last refreshed', case=False, na=False), 'Meta Data'].iloc[0]\n",
    "    info = info_df.loc[info_df['index'].str.contains('information', case=False, na=False), 'Meta Data'].iloc[0]\n",
    "    time_zone = info_df.loc[info_df['index'].str.contains('time zone', case=False, na=False), 'Meta Data'].iloc[0]\n",
    "\n",
    "    last_exec_dt = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    end_time = datetime.now()\n",
    "    tot_exec_time = int((end_time - start_time).total_seconds())\n",
    "    \n",
    "    \n",
    "    stock_data_df = info_df[~info_df['Time Series (Daily)'].isna()][['index', 'Time Series (Daily)']]\n",
    "    stock_data_df.columns = ['trade_dt', 'stock_info']\n",
    "\n",
    "    stock_data_df['trade_dt'] = pd.to_datetime(stock_data_df['trade_dt']).dt.date\n",
    "    stock_data_df['stock_info'] = stock_data_df['stock_info'].apply(json.dumps)\n",
    "    \n",
    "    \n",
    "    ## TO get incremental data\n",
    "    stock_data_df =  spark_get_inc_df(spark, url, stock_data_df, source_name, db_config)\n",
    "\n",
    "    ## To load the data into table\n",
    "    fn_insert_stock_date(stock_data_df, source_name, db_cnx)\n",
    "    \n",
    "    ## Update Souce Audit table\n",
    "    #update_source_audit(spark, url, db_config, source_name, last_refresh_dt, last_exec_dt, tot_exec_time, db_cnx)\n",
    "    fn_update_source_audit(source_name, last_refresh_dt, last_exec_dt, tot_exec_time, db_cnx)\n",
    "    \n",
    "    ## Insert data into task audit\n",
    "    fn_insert_task_audit(source_name,last_exec_dt,tot_exec_time, db_cnx)\n",
    "    \n",
    "    ## Insert meta information into source_info table\n",
    "    fn_insert_source_info(source_name, last_refresh_dt, time_zone, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSLA\n",
      "AMZN\n",
      "IBM\n",
      "MSFT\n",
      "AAPL\n"
     ]
    }
   ],
   "source": [
    "source_list = ['TSLA', 'AMZN', 'IBM', 'MSFT', 'AAPL']    \n",
    "for source_name in source_list:\n",
    "    print(source_name)\n",
    "    fn_fetch_data(source_name, spark, db_config, url, db_cnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_load_data(table_name, spark, db_config, url):\n",
    "    # url = \"jdbc:mysql://localhost:3306/dummy\"\n",
    "    \n",
    "    qry = f\"\"\"SELECT * \n",
    "    FROM stock_{table_name}_info\n",
    "    order by trade_dt desc\"\"\"\n",
    "    \n",
    "    df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"user\", db_config['user']) \\\n",
    "        .option(\"password\", db_config['password']) \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .option(\"query\", qry) \\\n",
    "        .load()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_strc_change(df, table_name):\n",
    "    df = df.withColumn(\"parsed_stock_info\", F.from_json(F.col(\"stock_info\"), json_schema))\n",
    "\n",
    "    df_transformed = df.select(\n",
    "        F.col(\"trade_dt\"),\n",
    "        F.col(\"parsed_stock_info.`1. open`\").alias(\"open\"),\n",
    "        F.col(\"parsed_stock_info.`2. high`\").alias(\"high\"),\n",
    "        F.col(\"parsed_stock_info.`3. low`\").alias(\"low\"),\n",
    "        F.col(\"parsed_stock_info.`4. close`\").alias(\"close\"),\n",
    "        F.col(\"parsed_stock_info.`5. volume`\").alias(\"volume\")\n",
    "    )\n",
    "    df_transformed = df_transformed.withColumn(\"company\", F.lit(table_name))\n",
    "    \n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_data_type_casting(df):\n",
    "    # Convert numeric fields to proper types\n",
    "    df = df.withColumn(\"open\", F.col(\"open\").cast(\"double\")) \\\n",
    "           .withColumn(\"high\", F.col(\"high\").cast(\"double\")) \\\n",
    "           .withColumn(\"low\", F.col(\"low\").cast(\"double\")) \\\n",
    "           .withColumn(\"close\", F.col(\"close\").cast(\"double\")) \\\n",
    "           .withColumn(\"volume\", F.col(\"volume\").cast(\"long\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = StructType([\n",
    "    StructField(\"1. open\", StringType(), True),\n",
    "    StructField(\"2. high\", StringType(), True),\n",
    "    StructField(\"3. low\", StringType(), True),\n",
    "    StructField(\"4. close\", StringType(), True),\n",
    "    StructField(\"5. volume\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_concat_tables(tables, spark, db_config ,url):\n",
    "    combined_df = None\n",
    "    \n",
    "    for table in tables:\n",
    "        try:\n",
    "            df = fn_load_data(table, spark, db_config, url)\n",
    "            \n",
    "            # Change structure\n",
    "            df = fn_strc_change(df, table)\n",
    "            \n",
    "            # Transform data\n",
    "            df = fn_data_type_casting(df)\n",
    "            \n",
    "            ## Data append on axis 0\n",
    "            if combined_df is None:\n",
    "                combined_df = df\n",
    "            else:\n",
    "                combined_df = combined_df.unionByName(df)\n",
    "            \n",
    "            print(f\"Processing completed for table -- stock_{table}_info\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing table stock_{table}_info: {e}\")\n",
    "            \n",
    "    print('\\n\\n')\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed for table -- stock_tsla_info\n",
      "Processing completed for table -- stock_amzn_info\n",
      "Processing completed for table -- stock_ibm_info\n",
      "Processing completed for table -- stock_msft_info\n",
      "Processing completed for table -- stock_aapl_info\n",
      "\n",
      "\n",
      "\n",
      "+----------+-------+--------+--------+------+---------+-------+\n",
      "|  trade_dt|   open|    high|     low| close|   volume|company|\n",
      "+----------+-------+--------+--------+------+---------+-------+\n",
      "|2024-11-26|  341.0|  346.96|  335.66|338.23| 61310388|   tsla|\n",
      "|2024-11-25| 360.14|  361.93|   338.2|338.59| 95890899|   tsla|\n",
      "|2024-11-22|341.085|  361.53|   337.7|352.56| 89140722|   tsla|\n",
      "|2024-11-21| 343.81|347.9899|  335.28|339.64| 57686586|   tsla|\n",
      "|2024-11-20|  345.0|346.5999|   334.3|342.03| 66340650|   tsla|\n",
      "|2024-11-19| 335.76|347.3799|  332.75| 346.0| 88852452|   tsla|\n",
      "|2024-11-18| 340.73|348.5499|  330.01|338.74|126547455|   tsla|\n",
      "|2024-11-15| 310.57|324.6799|  309.22|320.72|114440286|   tsla|\n",
      "|2024-11-14| 327.69|  329.98|  310.37|311.18|120726109|   tsla|\n",
      "|2024-11-13| 335.85|344.5999|   322.5|330.24|125405599|   tsla|\n",
      "|2024-11-12| 342.74|  345.84|  323.31|328.49|155726016|   tsla|\n",
      "|2024-11-11|  346.3|  358.64|   336.0| 350.0|210521625|   tsla|\n",
      "|2024-11-08| 299.14|  328.71|  297.66|321.22|204782763|   tsla|\n",
      "|2024-11-07| 288.89|  299.75|  285.52|296.91|117309232|   tsla|\n",
      "|2024-11-06| 284.67|  289.59|  275.62|288.53|165228710|   tsla|\n",
      "|2024-11-05| 247.34|255.2799|246.2101|251.44| 69282505|   tsla|\n",
      "|2024-11-04| 244.56|   248.9|  238.88|242.84| 68802354|   tsla|\n",
      "|2024-11-01|252.043|   254.0|  246.63|248.98| 57544757|   tsla|\n",
      "|2024-10-31| 257.99|  259.75|  249.25|249.85| 66575292|   tsla|\n",
      "|2024-10-30|258.035|  263.35|255.8201|257.55| 53993576|   tsla|\n",
      "+----------+-------+--------+--------+------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tables = ['tsla', 'amzn', 'ibm', 'msft', 'aapl']\n",
    "final_df = process_and_concat_tables(tables, spark, db_config, url)\n",
    "\n",
    "#Cache final_df after it's computed and before running any downstream operations:\n",
    "final_df.cache()\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|company|\n",
      "+-------+\n",
      "|   tsla|\n",
      "|   amzn|\n",
      "|    ibm|\n",
      "|   msft|\n",
      "|   aapl|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distinct_companies = final_df.select(\"company\").distinct()\n",
    "\n",
    "distinct_companies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_data_transformation(df):\n",
    "    df = df.withColumn(\"year_id\", year(\"trade_dt\"))  \n",
    "    df = df.withColumn(\"mth_id\", expr(\"year(trade_dt) * 100 + month(trade_dt)\"))\n",
    "    \n",
    "    df = df.filter(df['mth_id'] >= 201001)\n",
    "    \n",
    "    ## Changing Column order\n",
    "    col_order = ['year_id','mth_id', 'company'] + df.columns[:-3]\n",
    "    df = df.select(*col_order) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+----------+-------+--------+--------+------+---------+\n",
      "|year_id|mth_id|company|  trade_dt|   open|    high|     low| close|   volume|\n",
      "+-------+------+-------+----------+-------+--------+--------+------+---------+\n",
      "|   2024|202411|   tsla|2024-11-26|  341.0|  346.96|  335.66|338.23| 61310388|\n",
      "|   2024|202411|   tsla|2024-11-25| 360.14|  361.93|   338.2|338.59| 95890899|\n",
      "|   2024|202411|   tsla|2024-11-22|341.085|  361.53|   337.7|352.56| 89140722|\n",
      "|   2024|202411|   tsla|2024-11-21| 343.81|347.9899|  335.28|339.64| 57686586|\n",
      "|   2024|202411|   tsla|2024-11-20|  345.0|346.5999|   334.3|342.03| 66340650|\n",
      "|   2024|202411|   tsla|2024-11-19| 335.76|347.3799|  332.75| 346.0| 88852452|\n",
      "|   2024|202411|   tsla|2024-11-18| 340.73|348.5499|  330.01|338.74|126547455|\n",
      "|   2024|202411|   tsla|2024-11-15| 310.57|324.6799|  309.22|320.72|114440286|\n",
      "|   2024|202411|   tsla|2024-11-14| 327.69|  329.98|  310.37|311.18|120726109|\n",
      "|   2024|202411|   tsla|2024-11-13| 335.85|344.5999|   322.5|330.24|125405599|\n",
      "|   2024|202411|   tsla|2024-11-12| 342.74|  345.84|  323.31|328.49|155726016|\n",
      "|   2024|202411|   tsla|2024-11-11|  346.3|  358.64|   336.0| 350.0|210521625|\n",
      "|   2024|202411|   tsla|2024-11-08| 299.14|  328.71|  297.66|321.22|204782763|\n",
      "|   2024|202411|   tsla|2024-11-07| 288.89|  299.75|  285.52|296.91|117309232|\n",
      "|   2024|202411|   tsla|2024-11-06| 284.67|  289.59|  275.62|288.53|165228710|\n",
      "|   2024|202411|   tsla|2024-11-05| 247.34|255.2799|246.2101|251.44| 69282505|\n",
      "|   2024|202411|   tsla|2024-11-04| 244.56|   248.9|  238.88|242.84| 68802354|\n",
      "|   2024|202411|   tsla|2024-11-01|252.043|   254.0|  246.63|248.98| 57544757|\n",
      "|   2024|202410|   tsla|2024-10-31| 257.99|  259.75|  249.25|249.85| 66575292|\n",
      "|   2024|202410|   tsla|2024-10-30|258.035|  263.35|255.8201|257.55| 53993576|\n",
      "+-------+------+-------+----------+-------+--------+--------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df = fn_data_transformation(final_df)\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+------+-------+--------+----------+\n",
      "|company|mth_id|    open| close|   high|     low|tot_volume|\n",
      "+-------+------+--------+------+-------+--------+----------+\n",
      "|   aapl|201001|  201.08|214.01| 215.59|  190.25| 541749800|\n",
      "|   aapl|201002|  202.38|194.73| 205.17|  190.85| 384860000|\n",
      "|   aapl|201003|  235.49|208.99| 237.48|  205.45| 434077600|\n",
      "|   aapl|201004|  269.31|235.97| 272.46|  232.75| 441683200|\n",
      "|   aapl|201005|259.3881|266.35| 267.88|  199.25| 645809100|\n",
      "|   aapl|201006|  256.71|260.83| 279.01|   242.2| 594687600|\n",
      "|   aapl|201007|255.8925|248.48| 265.99|   239.6| 559632300|\n",
      "|   aapl|201008|  241.85|261.85| 264.28|  235.56| 342468600|\n",
      "|   aapl|201009|   289.0|250.33| 294.73|  246.28| 423211400|\n",
      "|   aapl|201010|  304.23|282.52|  319.0|  277.77| 436949200|\n",
      "|   aapl|201011|  313.54|304.18|  321.3|  297.76| 339574500|\n",
      "|   aapl|201012|  322.95| 316.4| 326.66|  314.89| 249044100|\n",
      "|   aapl|201101|   335.8|329.57|  348.6|324.8365| 387197700|\n",
      "|   aapl|201102|  351.24|345.03|  364.9|  337.72| 331998200|\n",
      "|   aapl|201103|  346.36|349.31| 361.67|  326.26| 403802100|\n",
      "|   aapl|201104|  346.78|344.56| 355.13|  320.16| 330493900|\n",
      "|   aapl|201105|   341.1|346.28| 351.83|  329.42| 246859300|\n",
      "|   aapl|201106|   334.7|345.51|352.132|   310.5| 330851800|\n",
      "|   aapl|201107|  387.64|343.26|  404.5|   334.2| 380498100|\n",
      "|   aapl|201108|  390.57|396.75|  399.5|  353.02| 576521400|\n",
      "+-------+------+--------+------+-------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grp_mth_df = final_df.groupBy(\"company\",\"mth_id\").agg(\n",
    "    F.first(\"open\").alias('open'),\n",
    "    F.last(\"close\").alias('close'),\n",
    "    F.max(\"high\").alias('high'),\n",
    "    F.min(\"low\").alias('low'),\n",
    "    F.sum(\"volume\").alias('tot_volume')\n",
    ")\n",
    "\n",
    "grp_mth_df = grp_mth_df.orderBy([\"company\", \"mth_id\"], ascending = True)\n",
    "grp_mth_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+------+-------+--------+----------+----------------+----------+\n",
      "|company|mth_id|    open| close|   high|     low|tot_volume|normalized_close|pct_change|\n",
      "+-------+------+--------+------+-------+--------+----------+----------------+----------+\n",
      "|   aapl|201001|  201.08|214.01| 215.59|  190.25| 541749800|           100.0|      NULL|\n",
      "|   aapl|201002|  202.38|194.73| 205.17|  190.85| 384860000|           90.99|     -9.01|\n",
      "|   aapl|201003|  235.49|208.99| 237.48|  205.45| 434077600|           97.65|      7.32|\n",
      "|   aapl|201004|  269.31|235.97| 272.46|  232.75| 441683200|          110.26|     12.91|\n",
      "|   aapl|201005|259.3881|266.35| 267.88|  199.25| 645809100|          124.46|     12.87|\n",
      "|   aapl|201006|  256.71|260.83| 279.01|   242.2| 594687600|          121.88|     -2.07|\n",
      "|   aapl|201007|255.8925|248.48| 265.99|   239.6| 559632300|          116.11|     -4.73|\n",
      "|   aapl|201008|  241.85|261.85| 264.28|  235.56| 342468600|          122.35|      5.38|\n",
      "|   aapl|201009|   289.0|250.33| 294.73|  246.28| 423211400|          116.97|      -4.4|\n",
      "|   aapl|201010|  304.23|282.52|  319.0|  277.77| 436949200|          132.01|     12.86|\n",
      "|   aapl|201011|  313.54|304.18|  321.3|  297.76| 339574500|          142.13|      7.67|\n",
      "|   aapl|201012|  322.95| 316.4| 326.66|  314.89| 249044100|          147.84|      4.02|\n",
      "|   aapl|201101|   335.8|329.57|  348.6|324.8365| 387197700|           154.0|      4.16|\n",
      "|   aapl|201102|  351.24|345.03|  364.9|  337.72| 331998200|          161.22|      4.69|\n",
      "|   aapl|201103|  346.36|349.31| 361.67|  326.26| 403802100|          163.22|      1.24|\n",
      "|   aapl|201104|  346.78|344.56| 355.13|  320.16| 330493900|           161.0|     -1.36|\n",
      "|   aapl|201105|   341.1|346.28| 351.83|  329.42| 246859300|          161.81|       0.5|\n",
      "|   aapl|201106|   334.7|345.51|352.132|   310.5| 330851800|          161.45|     -0.22|\n",
      "|   aapl|201107|  387.64|343.26|  404.5|   334.2| 380498100|          160.39|     -0.65|\n",
      "|   aapl|201108|  390.57|396.75|  399.5|  353.02| 576521400|          185.39|     15.58|\n",
      "+-------+------+--------+------+-------+--------+----------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(\"company\").orderBy(\"mth_id\")\n",
    "\n",
    "\n",
    "grp_mth_df = grp_mth_df.withColumn(\n",
    "    \"normalized_close\", \n",
    "    F.round((F.col(\"close\") / F.first(\"close\").over(window_spec)) * 100, 2)\n",
    ")\n",
    "\n",
    "# Operation 2: Calculate percentage change in 'close_price' for each 'company'\n",
    "grp_mth_df = grp_mth_df.withColumn(\n",
    "    \"pct_change\", \n",
    "    F.round((F.col(\"close\") - F.lag(\"close\").over(window_spec)) / F.lag(\"close\").over(window_spec) * 100, 2)\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "grp_mth_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+------+--------+-------+-----------+\n",
      "|company|year_id|   open| close|    high|    low| tot_volume|\n",
      "+-------+-------+-------+------+--------+-------+-----------+\n",
      "|   aapl|   2010| 322.95|214.01|  326.66| 190.25| 5393747400|\n",
      "|   aapl|   2011| 403.51|329.57|   426.7|  310.5| 4430690700|\n",
      "|   aapl|   2012| 510.53|411.23|  705.07|  409.0| 4713007300|\n",
      "|   aapl|   2013| 554.17|549.03|575.1358|  385.1| 3657913200|\n",
      "|   aapl|   2014| 112.82|553.13|  651.26|  89.65| 8734011985|\n",
      "|   aapl|   2015| 107.01|109.33|  134.54|   92.0|13064316775|\n",
      "|   aapl|   2016| 116.65|105.35|  118.69|  89.47| 9685871785|\n",
      "|   aapl|   2017| 170.52|116.15|   177.2| 114.76| 6687212823|\n",
      "|   aapl|   2018| 158.53|172.26|  233.47| 146.59| 8467093298|\n",
      "|   aapl|   2019| 289.93|157.92|  293.97|  142.0| 7086568153|\n",
      "|   aapl|   2020| 134.08|300.35|  515.14|  103.1|18552582945|\n",
      "|   aapl|   2021|178.085|129.41|  182.13| 116.21|22798348120|\n",
      "|   aapl|   2022| 128.41|182.01|  182.94| 125.87|22050192133|\n",
      "|   aapl|   2023|  193.9|125.07|  199.62| 124.17|14804889126|\n",
      "|   aapl|   2024| 233.58|185.64|  237.49|164.075|13346121923|\n",
      "|   amzn|   2010| 181.96| 133.9|  185.65|  105.8| 1836986500|\n",
      "|   amzn|   2011| 173.36|184.22|  246.71| 160.59| 1459978600|\n",
      "|   amzn|   2012| 243.75|179.03|  264.11|  172.0| 1049904100|\n",
      "|   amzn|   2013| 394.58|257.31|  405.63| 245.75|  747905700|\n",
      "|   amzn|   2014| 311.55|397.97|  408.06|  284.0| 1028972321|\n",
      "+-------+-------+-------+------+--------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grp_year_df = final_df.groupBy(\"company\",\"year_id\").agg(\n",
    "    F.first(\"open\").alias('open'),\n",
    "    F.last(\"close\").alias('close'),\n",
    "    F.max(\"high\").alias('high'),\n",
    "    F.min(\"low\").alias('low'),\n",
    "    F.sum(\"volume\").alias('tot_volume')\n",
    ")\n",
    "\n",
    "grp_year_df = grp_year_df.orderBy([\"company\",\"year_id\"], ascending = True)\n",
    "grp_year_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+------+--------+-------+-----------+----------------+----------+\n",
      "|company|year_id|   open| close|    high|    low| tot_volume|normalized_close|pct_change|\n",
      "+-------+-------+-------+------+--------+-------+-----------+----------------+----------+\n",
      "|   aapl|   2010| 322.95|214.01|  326.66| 190.25| 5393747400|           100.0|      NULL|\n",
      "|   aapl|   2011| 403.51|329.57|   426.7|  310.5| 4430690700|           154.0|      54.0|\n",
      "|   aapl|   2012| 510.53|411.23|  705.07|  409.0| 4713007300|          192.15|     24.78|\n",
      "|   aapl|   2013| 554.17|549.03|575.1358|  385.1| 3657913200|          256.54|     33.51|\n",
      "|   aapl|   2014| 112.82|553.13|  651.26|  89.65| 8734011985|          258.46|      0.75|\n",
      "|   aapl|   2015| 107.01|109.33|  134.54|   92.0|13064316775|           51.09|    -80.23|\n",
      "|   aapl|   2016| 116.65|105.35|  118.69|  89.47| 9685871785|           49.23|     -3.64|\n",
      "|   aapl|   2017| 170.52|116.15|   177.2| 114.76| 6687212823|           54.27|     10.25|\n",
      "|   aapl|   2018| 158.53|172.26|  233.47| 146.59| 8467093298|           80.49|     48.31|\n",
      "|   aapl|   2019| 289.93|157.92|  293.97|  142.0| 7086568153|           73.79|     -8.32|\n",
      "|   aapl|   2020| 134.08|300.35|  515.14|  103.1|18552582945|          140.34|     90.19|\n",
      "|   aapl|   2021|178.085|129.41|  182.13| 116.21|22798348120|           60.47|    -56.91|\n",
      "|   aapl|   2022| 128.41|182.01|  182.94| 125.87|22050192133|           85.05|     40.65|\n",
      "|   aapl|   2023|  193.9|125.07|  199.62| 124.17|14804889126|           58.44|    -31.28|\n",
      "|   aapl|   2024| 233.58|185.64|  237.49|164.075|13346121923|           86.74|     48.43|\n",
      "|   amzn|   2010| 181.96| 133.9|  185.65|  105.8| 1836986500|           100.0|      NULL|\n",
      "|   amzn|   2011| 173.36|184.22|  246.71| 160.59| 1459978600|          137.58|     37.58|\n",
      "|   amzn|   2012| 243.75|179.03|  264.11|  172.0| 1049904100|           133.7|     -2.82|\n",
      "|   amzn|   2013| 394.58|257.31|  405.63| 245.75|  747905700|          192.17|     43.72|\n",
      "|   amzn|   2014| 311.55|397.97|  408.06|  284.0| 1028972321|          297.21|     54.67|\n",
      "+-------+-------+-------+------+--------+-------+-----------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(\"company\").orderBy(\"year_id\")\n",
    "\n",
    "\n",
    "grp_year_df = grp_year_df.withColumn(\n",
    "    \"normalized_close\", \n",
    "    F.round((F.col(\"close\") / F.first(\"close\").over(window_spec)) * 100, 2)\n",
    ")\n",
    "\n",
    "#Calculate pct_change(%) in 'close' for each 'company'\n",
    "grp_year_df = grp_year_df.withColumn(\n",
    "    \"pct_change\", \n",
    "    F.round((F.col(\"close\") - F.lag(\"close\").over(window_spec)) / F.lag(\"close\").over(window_spec) * 100, 2)\n",
    ")\n",
    "\n",
    "grp_year_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm_env)",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
